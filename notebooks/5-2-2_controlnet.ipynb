{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f734c221-b1a8-4b18-8f34-ceaa8e2a7e6f",
      "metadata": {
        "id": "f734c221-b1a8-4b18-8f34-ceaa8e2a7e6f"
      },
      "source": [
        "# ControlNet の実装"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb42d95-7d45-4aac-92ca-2898bb367221",
      "metadata": {
        "id": "cfb42d95-7d45-4aac-92ca-2898bb367221"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/py-img-gen/python-image-generation/blob/main/notebooks/5-2-2_controlnet.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "664cebe7-3596-4ab8-b9ac-d66e0b65b5f1",
      "metadata": {
        "id": "664cebe7-3596-4ab8-b9ac-d66e0b65b5f1"
      },
      "source": [
        "## 準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "116b9549-ae46-4794-85df-0c4109daaaf1",
      "metadata": {
        "id": "116b9549-ae46-4794-85df-0c4109daaaf1"
      },
      "outputs": [],
      "source": [
        "!pip install -qq py-img-gen[controlnet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1f8552-9a58-4f4f-9ffe-16eb9bf8668f",
      "metadata": {
        "id": "ed1f8552-9a58-4f4f-9ffe-16eb9bf8668f"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "dtype = torch.float16\n",
        "seed = 19950815\n",
        "\n",
        "warnings.simplefilter(\"ignore\", FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4815fbe7-8fbb-4a1d-9cf2-5be94eb0b221",
      "metadata": {
        "id": "4815fbe7-8fbb-4a1d-9cf2-5be94eb0b221"
      },
      "source": [
        "## ControlNet を用いた Text-to-Image 生成"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1d1bd2d-11aa-422d-83fd-9efee542663e",
      "metadata": {
        "id": "b1d1bd2d-11aa-422d-83fd-9efee542663e"
      },
      "source": [
        "### オリジナル画像とエッジ画像の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a742e4bb-a6b5-47f0-aad7-d4a3fd58bc49",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "a742e4bb-a6b5-47f0-aad7-d4a3fd58bc49"
      },
      "outputs": [],
      "source": [
        "from diffusers.utils import load_image\n",
        "\n",
        "original_image = load_image(\n",
        "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
        ")\n",
        "\n",
        "original_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec80e3d5",
      "metadata": {
        "id": "ec80e3d5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL.Image import Image as PilImage\n",
        "\n",
        "\n",
        "def get_canny_image(\n",
        "    image: PilImage,\n",
        "    low_threshold: float,\n",
        "    high_threshold: float,\n",
        ") -> PilImage:\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    image_np = cv2.Canny(\n",
        "        image_np, low_threshold, high_threshold\n",
        "    )\n",
        "    # shape: (512, 512) -> (512, 512, 1)\n",
        "    image_np = image_np[:, :, None]\n",
        "    # shape: (512, 512, 1) -> (512, 512, 3)\n",
        "    image_np = np.concatenate(\n",
        "        [image_np, image_np, image_np], axis=2\n",
        "    )\n",
        "\n",
        "    canny_image = Image.fromarray(image_np)\n",
        "    return canny_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247ef783",
      "metadata": {
        "id": "247ef783"
      },
      "outputs": [],
      "source": [
        "from diffusers.utils import make_image_grid\n",
        "from PIL import Image\n",
        "\n",
        "canny_image = get_canny_image(\n",
        "    original_image, low_threshold=100, high_threshold=200\n",
        ")\n",
        "\n",
        "make_image_grid(\n",
        "    [original_image, canny_image], rows=1, cols=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e61bb530-7edc-4f30-ac7f-bdb890098a25",
      "metadata": {
        "id": "e61bb530-7edc-4f30-ac7f-bdb890098a25"
      },
      "source": [
        "### ControlNetModel と StableDiffusionControlNetPipeline の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9925251b-3036-4cb9-b325-077291de30ab",
      "metadata": {
        "id": "9925251b-3036-4cb9-b325-077291de30ab"
      },
      "outputs": [],
      "source": [
        "from diffusers import (\n",
        "    ControlNetModel,\n",
        "    StableDiffusionControlNetPipeline,\n",
        ")\n",
        "\n",
        "cnet_model_id = \"lllyasviel/sd-controlnet-canny\"\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    cnet_model_id, torch_dtype=dtype, use_safetensors=True\n",
        ")\n",
        "\n",
        "pipe_model_id = (\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
        ")\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    pipe_model_id,\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=dtype,\n",
        "    use_safetensors=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de21e997-ca7b-4d86-b422-85655247ab4e",
      "metadata": {
        "id": "de21e997-ca7b-4d86-b422-85655247ab4e"
      },
      "source": [
        "### ノイズスケジューラの変更"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c45c90-9648-4015-81c0-3fd2087cf4d6",
      "metadata": {
        "id": "d8c45c90-9648-4015-81c0-3fd2087cf4d6"
      },
      "outputs": [],
      "source": [
        "from diffusers import UniPCMultistepScheduler\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(\n",
        "    pipe.scheduler.config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6944f22-50f3-4fe2-b038-4f5d1c3d2496",
      "metadata": {
        "id": "e6944f22-50f3-4fe2-b038-4f5d1c3d2496"
      },
      "source": [
        "### CPU offload の有効化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1515fa8b-a3b4-43c7-8741-926e2b0e1948",
      "metadata": {
        "id": "1515fa8b-a3b4-43c7-8741-926e2b0e1948"
      },
      "outputs": [],
      "source": [
        "pipe.enable_model_cpu_offload()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1931fdca-a1e2-4bbd-998b-2fe96663416c",
      "metadata": {
        "id": "1931fdca-a1e2-4bbd-998b-2fe96663416c"
      },
      "source": [
        "### エッジ画像を元にした Text-to-Image 生成結果の表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11556c1-cd1b-4a5f-b50b-7f37d747be88",
      "metadata": {
        "id": "f11556c1-cd1b-4a5f-b50b-7f37d747be88"
      },
      "outputs": [],
      "source": [
        "output = pipe(\n",
        "    prompt=\"the mona lisa\",\n",
        "    image=canny_image,\n",
        "    generator=torch.manual_seed(seed),\n",
        ")\n",
        "output_image = output.images[0]\n",
        "images = [original_image, canny_image, output_image]\n",
        "make_image_grid(images, rows=1, cols=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eb736b4-90b4-49a0-b59f-1109f92ecdb6",
      "metadata": {
        "id": "6eb736b4-90b4-49a0-b59f-1109f92ecdb6"
      },
      "source": [
        "## ControlNet を用いた Image-to-Image 生成"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "819dfc62-3e75-44af-869f-a93b0132b3fd",
      "metadata": {
        "id": "819dfc62-3e75-44af-869f-a93b0132b3fd"
      },
      "source": [
        "### 深度マップの取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2caa152b-bfd9-4e87-9b86-b1f1ba6cc59a",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "2caa152b-bfd9-4e87-9b86-b1f1ba6cc59a"
      },
      "outputs": [],
      "source": [
        "image = load_image(\n",
        "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg\"\n",
        ")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42bbdeb8",
      "metadata": {
        "id": "42bbdeb8"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "from transformers.pipelines import DepthEstimationPipeline\n",
        "\n",
        "\n",
        "def get_image_depth_map(\n",
        "    image: PilImage,\n",
        "    depth_estimator: Optional[\n",
        "        DepthEstimationPipeline\n",
        "    ] = None,\n",
        ") -> PilImage:\n",
        "    depth_estimator = depth_estimator or pipeline(\n",
        "        task=\"depth-estimation\", model=\"Intel/dpt-large\"\n",
        "    )\n",
        "    output = depth_estimator(image)\n",
        "    return output[\"depth\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45243a53",
      "metadata": {
        "id": "45243a53"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "def get_tensor_depth_map(\n",
        "    depth_image: PilImage,\n",
        "    transform_to_batch: bool = True,\n",
        ") -> torch.Tensor:\n",
        "    depth_image_np = np.array(depth_image)\n",
        "    # shape: (768, 768 -> (768, 768, 1)\n",
        "    depth_image_np = depth_image_np[:, :, None]\n",
        "    # shape: (768, 768, 1) -> (768, 768, 3)\n",
        "    depth_image_np = np.concatenate(\n",
        "        [depth_image_np, depth_image_np, depth_image_np],\n",
        "        axis=2,\n",
        "    )\n",
        "\n",
        "    depth_image_th = (\n",
        "        torch.from_numpy(depth_image_np).float() / 255.0\n",
        "    )\n",
        "    # shape: (768, 768, 3) -> (3, 768, 768)\n",
        "    depth_image_th = depth_image_th.permute(2, 0, 1)\n",
        "\n",
        "    if not transform_to_batch:\n",
        "        return depth_image_th\n",
        "\n",
        "    # shape: (3, 768, 768) -> (1, 3, 768, 768)\n",
        "    depth_image_th = depth_image_th.unsqueeze(dim=0)\n",
        "\n",
        "    return depth_image_th"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e7aa127",
      "metadata": {
        "id": "0e7aa127"
      },
      "outputs": [],
      "source": [
        "depth_map_pl = get_image_depth_map(image)\n",
        "depth_map_th = get_tensor_depth_map(depth_map_pl)\n",
        "\n",
        "# `dtype` の変換と GPU device への移動\n",
        "depth_map_th = depth_map_th.to(device=device, dtype=dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4793ea23-2f1d-40b7-86a5-cdd99c15fad6",
      "metadata": {
        "id": "4793ea23-2f1d-40b7-86a5-cdd99c15fad6"
      },
      "source": [
        "### ControlNetModel と StableDiffusionControlNetImg2ImgPipeline の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3b4600-fddf-465f-a1ec-0740c78afd4d",
      "metadata": {
        "id": "7a3b4600-fddf-465f-a1ec-0740c78afd4d"
      },
      "outputs": [],
      "source": [
        "from diffusers import (\n",
        "    StableDiffusionControlNetImg2ImgPipeline,\n",
        ")\n",
        "\n",
        "cnet_model_id = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    cnet_model_id, torch_dtype=dtype, use_safetensors=True\n",
        ")\n",
        "\n",
        "pipe_model_id = (\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
        ")\n",
        "pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n",
        "    pipe_model_id,\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=dtype,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(\n",
        "    pipe.scheduler.config\n",
        ")\n",
        "\n",
        "pipe.enable_model_cpu_offload()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf794c8-cdae-447d-aa60-6f18f11b432c",
      "metadata": {
        "id": "abf794c8-cdae-447d-aa60-6f18f11b432c"
      },
      "source": [
        "### 深度マップを元にした Image-to-Image 生成結果の表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12aa91f0-1adf-4446-8db9-5ac1d76ad08f",
      "metadata": {
        "id": "12aa91f0-1adf-4446-8db9-5ac1d76ad08f"
      },
      "outputs": [],
      "source": [
        "output = pipe(\n",
        "    prompt=\"lego batman and robin\",\n",
        "    image=image,\n",
        "    control_image=depth_map_th,\n",
        "    generator=torch.manual_seed(seed),\n",
        ").images[0]\n",
        "\n",
        "make_image_grid(\n",
        "    [image, depth_map_pl, output], rows=1, cols=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138fc33e-a074-4203-bd24-fa6c82a065a4",
      "metadata": {
        "id": "138fc33e-a074-4203-bd24-fa6c82a065a4"
      },
      "source": [
        "## ControlNet を用いた Inpainting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff60f9e9-7421-491e-8e2c-3c2e82cb666e",
      "metadata": {
        "id": "ff60f9e9-7421-491e-8e2c-3c2e82cb666e"
      },
      "source": [
        "### 初期画像とマスク画像の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21147bf7-b232-4542-a625-ce0fb3a23457",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "21147bf7-b232-4542-a625-ce0fb3a23457"
      },
      "outputs": [],
      "source": [
        "init_image = load_image(\n",
        "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg\"\n",
        ")\n",
        "init_image = init_image.resize((512, 512))\n",
        "\n",
        "mask_image = load_image(\n",
        "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg\"\n",
        ")\n",
        "mask_image = mask_image.resize((512, 512))\n",
        "make_image_grid([init_image, mask_image], rows=1, cols=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5811e2c3-7d68-4764-a73a-4ef0c77b3b7e",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "5811e2c3-7d68-4764-a73a-4ef0c77b3b7e"
      },
      "source": [
        "### Inpainting を制御する画像を作成する関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b26de2-46b0-456e-ba9b-867a4a72ba7e",
      "metadata": {
        "id": "34b26de2-46b0-456e-ba9b-867a4a72ba7e"
      },
      "outputs": [],
      "source": [
        "def make_inpaint_condition(\n",
        "    image: PilImage, image_mask: PilImage\n",
        ") -> torch.Tensor:\n",
        "    image = image.convert(\"RGB\")\n",
        "    image_np = np.array(image, dtype=np.float32)\n",
        "    image_np /= 255.0\n",
        "\n",
        "    image_mask = image_mask.convert(\"L\")\n",
        "    image_mask_np = np.array(image_mask, dtype=np.float32)\n",
        "    image_mask_np /= 255.0\n",
        "\n",
        "    assert image_np.shape[0:1] == image_mask_np.shape[0:1]\n",
        "    image_np[\n",
        "        image_mask_np > 0.5\n",
        "    ] = -1.0  # マスクされたピクセルとする\n",
        "\n",
        "    # shape: (512, 512, 3) -> (1, 512, 512, 3)\n",
        "    image_np = image_np[None, :, :, :]\n",
        "    # shape: (1, 3, 512, 512)\n",
        "    image_np = image_np.transpose(0, 3, 1, 2)\n",
        "\n",
        "    image_th = torch.from_numpy(image_np)\n",
        "    return image_th\n",
        "\n",
        "\n",
        "control_image = make_inpaint_condition(\n",
        "    init_image, mask_image\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a48a4d63-4232-4224-b949-61677e1f0120",
      "metadata": {
        "id": "a48a4d63-4232-4224-b949-61677e1f0120"
      },
      "source": [
        "### ControlNetModel と StableDiffusionControlNetInpaintPipeline の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a80dc1-58d4-4c98-a0dd-60588f19c315",
      "metadata": {
        "id": "45a80dc1-58d4-4c98-a0dd-60588f19c315"
      },
      "outputs": [],
      "source": [
        "from diffusers import (\n",
        "    StableDiffusionControlNetInpaintPipeline,\n",
        ")\n",
        "\n",
        "cnet_model_id = \"lllyasviel/control_v11p_sd15_inpaint\"\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    cnet_model_id, torch_dtype=dtype, use_safetensors=True\n",
        ")\n",
        "\n",
        "pipe_model_id = (\n",
        "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
        ")\n",
        "pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
        "    pipe_model_id,\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=dtype,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(\n",
        "    pipe.scheduler.config\n",
        ")\n",
        "pipe.enable_model_cpu_offload()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1ed15b5-95da-4299-a36d-d92e361866aa",
      "metadata": {
        "id": "f1ed15b5-95da-4299-a36d-d92e361866aa"
      },
      "source": [
        "### マスク画像を元にした ControlNet による inpainting 結果の表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfad64f3-924b-4fcd-a5a9-62d5df6bb0bd",
      "metadata": {
        "id": "bfad64f3-924b-4fcd-a5a9-62d5df6bb0bd"
      },
      "outputs": [],
      "source": [
        "prompt = \"corgi face with large ears, detailed, pixar, animated, disney\"\n",
        "\n",
        "output = pipe(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=20,\n",
        "    eta=1.0,\n",
        "    image=init_image,\n",
        "    mask_image=mask_image,\n",
        "    control_image=control_image,\n",
        "    generator=torch.manual_seed(seed),\n",
        ").images[0]\n",
        "\n",
        "make_image_grid(\n",
        "    [init_image, mask_image, output], rows=1, cols=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a517d39-e8ad-4e8b-bab2-63fc5a852a4d",
      "metadata": {
        "id": "9a517d39-e8ad-4e8b-bab2-63fc5a852a4d"
      },
      "source": [
        "## 複数の条件を考慮可能な MultiControlNet による画像生成"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fadbb008-3707-4731-a027-52f7ccfd480b",
      "metadata": {
        "id": "fadbb008-3707-4731-a027-52f7ccfd480b"
      },
      "source": [
        "### オリジナル画像とエッジ画像の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680c1a69-5afe-4024-9550-b1aa8f917c25",
      "metadata": {
        "id": "680c1a69-5afe-4024-9550-b1aa8f917c25"
      },
      "outputs": [],
      "source": [
        "original_image = load_image(\n",
        "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\"\n",
        ")\n",
        "image = np.array(original_image)\n",
        "\n",
        "low_threshold, high_threshold = 100, 200\n",
        "\n",
        "image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "\n",
        "# 姿勢情報が重ねられる画像の中央列をゼロにする\n",
        "zero_start = image.shape[1] // 4\n",
        "zero_end = zero_start + image.shape[1] // 2\n",
        "image[:, zero_start:zero_end] = 0\n",
        "\n",
        "image = image[:, :, None]\n",
        "image = np.concatenate([image, image, image], axis=2)\n",
        "canny_image = Image.fromarray(image)\n",
        "make_image_grid(\n",
        "    [original_image, canny_image], rows=1, cols=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d5d48d-e6a6-4dd7-b1c9-0e02eb189a6a",
      "metadata": {
        "id": "f3d5d48d-e6a6-4dd7-b1c9-0e02eb189a6a"
      },
      "source": [
        "### 姿勢情報の取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3cad7d1-4ec8-48df-82c9-d781cada33f3",
      "metadata": {
        "id": "f3cad7d1-4ec8-48df-82c9-d781cada33f3"
      },
      "outputs": [],
      "source": [
        "from controlnet_aux import OpenposeDetector\n",
        "\n",
        "openpose = OpenposeDetector.from_pretrained(\n",
        "    \"lllyasviel/ControlNet\"\n",
        ")\n",
        "original_image = load_image(\n",
        "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n",
        ")\n",
        "openpose_image = openpose(original_image)\n",
        "make_image_grid(\n",
        "    [original_image, openpose_image], rows=1, cols=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b70610-edae-492b-95aa-bd028b3463ee",
      "metadata": {
        "id": "26b70610-edae-492b-95aa-bd028b3463ee"
      },
      "source": [
        "### 複数の ControlNet を StableDiffusionControlNetPipeline へ渡す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192a00e0-c7b3-453c-ba0c-9c49d1e952ae",
      "metadata": {
        "id": "192a00e0-c7b3-453c-ba0c-9c49d1e952ae"
      },
      "outputs": [],
      "source": [
        "cn1 = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-openpose\", torch_dtype=dtype\n",
        ")\n",
        "cn2 = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-canny\", torch_dtype=dtype\n",
        ")\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    pipe_model_id, controlnet=[cn1, cn2], torch_dtype=dtype\n",
        ")\n",
        "pipe.scheduer = UniPCMultistepScheduler.from_config(\n",
        "    pipe.scheduler.config\n",
        ")\n",
        "\n",
        "pipe.enable_model_cpu_offload()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da75cb0c-2a18-44f9-8f33-931135257e69",
      "metadata": {
        "id": "da75cb0c-2a18-44f9-8f33-931135257e69"
      },
      "source": [
        "### 複数の ControlNet による画像生成の結果の表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c62d4cc-06fd-4943-b4bd-93dd70ed7d8e",
      "metadata": {
        "id": "5c62d4cc-06fd-4943-b4bd-93dd70ed7d8e"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    \"a giant standing in a fantasy landscape, best quality\"\n",
        ")\n",
        "negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
        "\n",
        "control_images = [openpose_image, canny_image]\n",
        "\n",
        "image = pipe(\n",
        "    prompt,\n",
        "    image=control_images,\n",
        "    num_inference_steps=25,\n",
        "    generator=torch.manual_seed(seed),\n",
        "    negative_prompt=negative_prompt,\n",
        "    controlnet_conditioning_scale=[1.0, 0.8],\n",
        ").images[0]\n",
        "\n",
        "make_image_grid(\n",
        "    [image, openpose_image, canny_image], rows=1, cols=3\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}