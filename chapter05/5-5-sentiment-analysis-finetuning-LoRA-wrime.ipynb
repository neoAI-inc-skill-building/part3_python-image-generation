{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93ccd385",
      "metadata": {
        "id": "93ccd385"
      },
      "source": [
        "# 第5章 大規模言語モデルのファインチューニング"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**注意**\n",
        "2023/7/28現在、MARC-jaのデータセットの配布元のリンクが切れており、書籍上の5.2、5.3、5.5.4節に掲載されているコードにおいて、データセット読み込みの箇所でエラーが出る状態です。こちらのノートブックは、MARC-jaと同様の感情分析のデータセットであるWRIMEを用いて書籍と同様のコードを実行するために用意されています。"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "JVAyPjrz8R1G"
      },
      "id": "JVAyPjrz8R1G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 メモリ効率の良いファインチューニング"
      ],
      "metadata": {
        "id": "DwQFBLLpU5PN"
      },
      "id": "DwQFBLLpU5PN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5.4 LoRAチューニング"
      ],
      "metadata": {
        "id": "SWuDdHxVU6MV"
      },
      "id": "SWuDdHxVU6MV"
    },
    {
      "cell_type": "markdown",
      "id": "e451487d",
      "metadata": {
        "id": "e451487d"
      },
      "source": [
        "#### 環境の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95812908",
      "metadata": {
        "id": "95812908"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[ja,torch] \"datasets<4.0.0,>=2.14.6\" matplotlib japanize-matplotlib peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f016fa8a",
      "metadata": {
        "id": "f016fa8a"
      },
      "outputs": [],
      "source": [
        "from transformers.trainer_utils import set_seed\n",
        "\n",
        "# 乱数シードを42に固定\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f09cfa2",
      "metadata": {
        "id": "5f09cfa2"
      },
      "source": [
        "#### データセットの準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d4ba4c0",
      "metadata": {
        "id": "6d4ba4c0"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Hugging Face Hub上のllm-book/wrime-sentimentのリポジトリから\n",
        "# データを読み込む\n",
        "train_dataset = load_dataset(\"llm-book/wrime-sentiment\", split=\"train\", trust_remote_code=True)\n",
        "valid_dataset = load_dataset(\"llm-book/wrime-sentiment\", split=\"validation\")\n",
        "# pprintで見やすく表示する\n",
        "pprint(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a61ece7",
      "metadata": {
        "id": "1a61ece7"
      },
      "outputs": [],
      "source": [
        "pprint(train_dataset.features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615990b5",
      "metadata": {
        "id": "615990b5"
      },
      "source": [
        "#### トークナイザ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb39540",
      "metadata": {
        "id": "5bb39540"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Hugging Face Hub上のモデル名を指定\n",
        "model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
        "# モデル名からトークナイザを読み込む\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# トークナイザのクラス名を確認\n",
        "print(type(tokenizer).__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99db6435",
      "metadata": {
        "id": "99db6435"
      },
      "outputs": [],
      "source": [
        "tokenizer.tokenize(\"これはテストです。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4300d90",
      "metadata": {
        "id": "a4300d90"
      },
      "outputs": [],
      "source": [
        "encoded_input = tokenizer(\"これはテストです。\")\n",
        "# 出力されたオブジェクトのクラスを表示\n",
        "print(type(encoded_input).__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aebd3c8",
      "metadata": {
        "id": "2aebd3c8"
      },
      "outputs": [],
      "source": [
        "pprint(encoded_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27af9b91",
      "metadata": {
        "id": "27af9b91"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4574c3d",
      "metadata": {
        "id": "c4574c3d"
      },
      "source": [
        "#### データセット統計の可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743767a1",
      "metadata": {
        "id": "743767a1"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import japanize_matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "plt.rcParams[\"font.size\"] = 18  # 文字サイズを大きくする\n",
        "\n",
        "def visualize_text_length(dataset: Dataset):\n",
        "    \"\"\"データセット中のテキストのトークン数の分布をグラフとして描画\"\"\"\n",
        "    # データセット中のテキストの長さを数える\n",
        "    length_counter = Counter()\n",
        "    for data in tqdm(dataset):\n",
        "        length = len(tokenizer.tokenize(data[\"sentence\"]))\n",
        "        length_counter[length] += 1\n",
        "    # length_counterの値から棒グラフを描画する\n",
        "    plt.bar(length_counter.keys(), length_counter.values(), width=1.0)\n",
        "    plt.xlabel(\"トークン数\")\n",
        "    plt.ylabel(\"事例数\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_text_length(train_dataset)\n",
        "visualize_text_length(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c7f3ba",
      "metadata": {
        "id": "42c7f3ba"
      },
      "outputs": [],
      "source": [
        "def visualize_labels(dataset: Dataset):\n",
        "    \"\"\"データセット中のラベル分布をグラフとして描画\"\"\"\n",
        "    # データセット中のラベルの数を数える\n",
        "    label_counter = Counter()\n",
        "    for data in dataset:\n",
        "        label_id = data[\"label\"]\n",
        "        label_name = dataset.features[\"label\"].names[label_id]\n",
        "        label_counter[label_name] += 1\n",
        "    # label_counterを棒グラフとして描画する\n",
        "    plt.bar(label_counter.keys(), label_counter.values(), width=1.0)\n",
        "    plt.xlabel(\"ラベル\")\n",
        "    plt.ylabel(\"事例数\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_labels(train_dataset)\n",
        "visualize_labels(valid_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9981e005",
      "metadata": {
        "id": "9981e005"
      },
      "source": [
        "#### データセットの前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db4deb87",
      "metadata": {
        "id": "db4deb87"
      },
      "outputs": [],
      "source": [
        "from transformers import BatchEncoding\n",
        "\n",
        "def preprocess_text_classification(\n",
        "    example: dict[str, str | int]\n",
        ") -> BatchEncoding:\n",
        "    \"\"\"文書分類の事例のテキストをトークナイズし、IDに変換\"\"\"\n",
        "    encoded_example = tokenizer(example[\"sentence\"], max_length=512)\n",
        "    # モデルの入力引数である\"labels\"をキーとして格納する\n",
        "    encoded_example[\"labels\"] = example[\"label\"]\n",
        "    return encoded_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d1d1a2",
      "metadata": {
        "id": "13d1d1a2"
      },
      "outputs": [],
      "source": [
        "encoded_train_dataset = train_dataset.map(\n",
        "    preprocess_text_classification,\n",
        "    remove_columns=train_dataset.column_names,\n",
        ")\n",
        "encoded_valid_dataset = valid_dataset.map(\n",
        "    preprocess_text_classification,\n",
        "    remove_columns=valid_dataset.column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e35135a4",
      "metadata": {
        "id": "e35135a4"
      },
      "outputs": [],
      "source": [
        "print(encoded_train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3a07a6",
      "metadata": {
        "id": "2a3a07a6"
      },
      "source": [
        "#### ミニバッチ構築"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6203489d",
      "metadata": {
        "id": "6203489d"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3041d814",
      "metadata": {
        "id": "3041d814"
      },
      "outputs": [],
      "source": [
        "batch_inputs = data_collator(encoded_train_dataset[0:4])\n",
        "pprint({name: tensor.size() for name, tensor in batch_inputs.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81822ce0",
      "metadata": {
        "id": "81822ce0"
      },
      "source": [
        "#### モデルの準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5e5f59",
      "metadata": {
        "id": "fd5e5f59"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "class_label = train_dataset.features[\"label\"]\n",
        "label2id = {label: id for id, label in enumerate(class_label.names)}\n",
        "id2label = {id: label for id, label in enumerate(class_label.names)}\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=class_label.num_classes,\n",
        "    label2id=label2id,  # ラベル名からIDへの対応を指定\n",
        "    id2label=id2label,  # IDからラベル名への対応を指定\n",
        ")\n",
        "print(type(base_model).__name__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import peft\n",
        "\n",
        "peft_config = peft.LoraConfig(\n",
        "    task_type=peft.TaskType.SEQ_CLS,  # モデルが解くタスクのタイプを指定\n",
        "    r=8,  # 差分行列のランク\n",
        "    lora_alpha=32,  #  LoRA層の出力のスケールを調節するハイパーパラメータ\n",
        "    lora_dropout=0.1,  # LoRA層に適用するドロップアウト\n",
        "    inference_mode=False,  # 推論モードの設定（今回は学習時なのでFalse）\n",
        ")\n",
        "model = peft.get_peft_model(base_model, peft_config)\n",
        "print(type(model).__name__)"
      ],
      "metadata": {
        "id": "HDx_-7JElPU_"
      },
      "id": "HDx_-7JElPU_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "7GpLhX4dlRV2"
      },
      "id": "7GpLhX4dlRV2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3ef02e",
      "metadata": {
        "id": "bc3ef02e"
      },
      "outputs": [],
      "source": [
        "print(model.forward(**data_collator(encoded_train_dataset[0:4])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb81a95",
      "metadata": {
        "id": "0bb81a95"
      },
      "source": [
        "#### 訓練の実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54cf1b63",
      "metadata": {
        "id": "54cf1b63"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output_wrime_lora\",  # 結果の保存フォルダ\n",
        "    per_device_train_batch_size=32,  # 訓練時のバッチサイズ\n",
        "    per_device_eval_batch_size=32,  # 評価時のバッチサイズ\n",
        "    learning_rate=2e-4,  # 学習率\n",
        "    lr_scheduler_type=\"linear\",  # 学習率スケジューラの種類\n",
        "    warmup_ratio=0.1,  # 学習率のウォームアップの長さを指定\n",
        "    num_train_epochs=3,  # エポック数\n",
        "    save_strategy=\"epoch\",  # チェックポイントの保存タイミング\n",
        "    logging_strategy=\"epoch\",  # ロギングのタイミング\n",
        "    eval_strategy=\"epoch\",  # 検証セットによる評価のタイミング\n",
        "    load_best_model_at_end=True,  # 訓練後に開発セットで最良のモデルをロード\n",
        "    metric_for_best_model=\"accuracy\",  # 最良のモデルを決定する評価指標\n",
        "    fp16=True,  # 自動混合精度演算の有効化\n",
        "    report_to=\"none\",  # 外部ツールへのログを無効化\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0323ffd8",
      "metadata": {
        "id": "0323ffd8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_accuracy(\n",
        "    eval_pred: tuple[np.ndarray, np.ndarray]\n",
        ") -> dict[str, float]:\n",
        "    \"\"\"予測ラベルと正解ラベルから正解率を計算\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    # predictionsは各ラベルについてのスコア\n",
        "    # 最もスコアの高いインデックスを予測ラベルとする\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871c234d",
      "metadata": {
        "id": "871c234d"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=encoded_train_dataset,\n",
        "    eval_dataset=encoded_valid_dataset,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_accuracy,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00d79e9c",
      "metadata": {
        "id": "00d79e9c"
      },
      "source": [
        "#### 訓練後のモデルの評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f6737b3",
      "metadata": {
        "id": "1f6737b3"
      },
      "outputs": [],
      "source": [
        "# 検証セットでモデルを評価\n",
        "eval_metrics = trainer.evaluate(encoded_valid_dataset)\n",
        "pprint(eval_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwT8LbmX6VyF"
      },
      "id": "qwT8LbmX6VyF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}